[
  {
    "path": "posts/2021-07-17-2021-07-15-final-report/",
    "title": "Final Report",
    "description": "Summary of what I did, and where to go.",
    "author": [
      {
        "name": "Robert Hess",
        "url": {}
      }
    ],
    "date": "2021-07-15",
    "categories": [],
    "contents": "\r\nBackground\r\nFor this project, I wanted to us NLP (Natural Language Processing) to determine what effects the News had on the Forex Market.\r\nWhat is Forex?\r\nForex is short for Foreign Exchange. The Forex market is the trading of currencies around the world. As the currencies change in power one to another people can trade their currencies for profit. Forex uses what is called pairs. A pair is the comparison of two currencies. All the following analysis was using the pair United States Dollar / Canadian Dollar (USDCAD).\r\nWhat are Candles?\r\n Candles are a measure of time. Each candle represents a time period. It can be 30 Minutes, 3 Hours, 1 Day, 1 Year, etc… If the candle is red the open and close prices are as shown (at the top and bottom of the body respectively). If the candle is green that means the open and close prices are flipped from what is shown (Open on the bottom of the body and Close at the top).\r\nWhat is an ATR?\r\nAn ATR (Average True Range) is a measurement of the movement of the market. It doesn’t represent direction, just the amount that the market is moving. It is similar to the variance of the market. A range is calculated by taking the High and subtracting the low from it. The ATR is the mean of these ranges of a certain number of candles. In this analysis a 14 candle ATR was used.\r\nSteps\r\nGather Data\r\nNews\r\nFor this project I decided to use the Federal Reserve Statements as my news sources. An example of one of these can be found here.\r\nThese statments were directly downloaded from the Federal Reserve Website. The dates and times were determined from Forex Factory.\r\nCandles\r\nThe candles that were used in this project were downloaded from Oanda using a proprietary downloader from the company Liberty Dynamic Consultants that I was able to use for this project.\r\nBuild Data\r\nPrepare news for NLP\r\nIn order to prepare the Federal Reserve Statements for NLP I had strip it from the downloaded HTMLs and then pre-process it. A computer can’t really read or understand language. The best that it can do is make correlations between words and a classification. A major problem is that Hello and hello are not seen as the same thing to a computer due to the capital h. capitals and many other things can make a computer need considerably more data. Here are some of the stepsI followed to pre-process the raw text.\r\nFind a way to grab just the wanted text from the HTMLs.\r\nAdd spaces to split words that got combined in the stripping process.\r\nRemove punctuation.\r\nRemove unwanted white space. (Sometimes there were double spaces in the middle of the text and extra spaces at the beginning and end. These needed to be removed because ‘hello’ and ’ hello’ are not considered the same thing to a computer.)\r\nMake all words be lower case.\r\nChange all numbers to a uniform “num” word. (This was done because the numbers taken out of context are meaningless. If we want to keep the numbers, they should be seen as a separate entity and labeled as what the number means rather than being labeled as a word.)\r\nRemove Stopwords. (Some words have little to no meaning such as a and the. These words are called stopwords and should be removed as to not clutter the data and cause the need for more data to be higher.)\r\nLemmatize or Stem the words. This causes words that have the same root word to be considered as the same word. Stemming does this as an algorithm, therefore it doesn’t do quite as good, but it is quite fast. Problems that may arise is that words like mean and meaning end up both as mean when stemming and goose and geese are still seen as completely different words. Lemmatizing on the other hand is slower, but it will look up what the root word for each word and turn all the words into their dictionary base word. This means that goose and geese are both seen as goose. Lemmatizing is slower than Stemming. These considerations should be used when making the decision on which to use.\r\nCalculate ATR\r\nIn order to calculate this take the high of a candle and subtract the low. Add this number to all the other high-low and divide by the number of candles. Example of doing this in R: candles_with_atr_30_min <- candles_filtered %>% arrange(date) %>% mutate(row_num = row_number()) %>% rowwise() %>% mutate(atr = atr_calc(candles_filtered[(abs(row_num-14)):abs(row_num-1),3], candles_filtered[abs(row_num-14):abs(row_num-1),4]))\r\natr_calc <- function(high.df, low.df){ return(sum(high.df - low.df) / nrow(high.df)) }\r\nATR Discovery\r\nAs I was analyzing the ATR and the candles I discovered that the range of the news candle was often lower than the ATR, which surprised me. I dug a little deeper to see how much lower they were, and if it was possibly an insignificant amount. To do this, I took the ranges of the 14 ATR candles and ordered them least to greatest. Then I placed the News Candle in the order where it should be and gave it a rank according to its placement (1-15, 1 being the smallest candle and 15 being the largest). If the news had no affect on the movement of the market we would have expected the news candle’s ranking to be evenly spread between 1 and 15 (1 would have been just as likely as 15). This is not what I found. The news candles are very clustered to the left side, many being the smallest or second smallest candles as shown in the graph below. This means that the market takes a relative pause for up to three hours after a news event.\r\nRankingNLP Discovery\r\nAs I was preparing the text for NLP I discovered that I had too many words and not enough statements. In order for a machine learning project ot be effective, the number of columns needs to be smaller than the number of rows. The problem is that the number of columns that I had was 8 to 10 times larger than the number of rows. This made it so that I was unable to pursue an NLP model with just the Federal Reserve Statments that are released 8 times a year.\r\nWhere to go\r\nIn Order the actually do NLP, I will need to obtain more news data dn not rely and a single type. This could cause other complications, but it is necessary.\r\nThere are techniques (such as getting rid of words only ever used once) to limit the amount of words that are being considered within the NLP which might allow for a model to be built with fewer rows.\r\nInvestigation into why the market stops moving (is it because the news is opposite the current trend of the market? What other reasons could be the cause?)\r\nBecause the market doesn’t move in the time frame that I looked at, money cannot be made. In order to make money the market must be moving. Discover what happens once the market starts moving as well as when it starts moving again.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-07-17T10:54:29-06:00",
    "input_file": "2021-07-15-final-report.knit.md"
  },
  {
    "path": "posts/2021-05-19-text-candles-and-dates/",
    "title": "Text, Candles, and Dates",
    "description": "Data Collection",
    "author": [
      {
        "name": "Robert Hess",
        "url": {}
      }
    ],
    "date": "2021-05-19",
    "categories": [],
    "contents": "\r\nData Collection\r\nThis project requires data from many different sources to be combined together. I needed to find and obtain raw text from the Federal Reserve, Candle Data from Oanda, and Dates and Times from forexfactory.\r\nFederal Reserve & Forex Factory\r\nIn order to find which news events I wanted to use, I went through the Forex Factory and found one of the events on their calender. These FOMC Statements have a naming pattern on the Federal Reserve Website. They are marked by date. I found a list of dates for all of the statements that happened since 2007. I used python to download the HTMLs and then stripped the Statement text from the HTMLS using some simple regex.\r\nOanda\r\nOanda can provide their candle data to those who have their API. I used a candle downloader from the company Liberty Dynamic Consultants in which I am a stakeholder and have access to their technology. Getting the Postgresql database setup on my computer had a few settings issues that made it difficult.\r\nData Wrangling\r\nRegexes\r\nLook Aheads and Look Behinds This regex finds any “.” or lowercase letter that is followed by an uppercase letter. ([.a-z](?=[A-Z])) This regex finds 2 or more whitespaces together that have words ahead and digits behind. ((?<![\\d\\s])\\s\\s+(?=\\w))\r\nRemove punct This regex finds all none word and none space and non “-” characters and deletes them. re.sub(r'[^\\w\\s-]', '', x)\r\nAverage True Range (ATR)\r\nThe ATR is the average amount that the market is moving. In order to calculate this take the high of a candle and subtract the low. Add this number to all the other high-low and divide by the number of candles. Example of doing this in R: candles_with_atr_30_min <- candles_filtered %>% arrange(date) %>% mutate(row_num = row_number()) %>% rowwise() %>% mutate(atr = atr_calc(candles_filtered[(abs(row_num-14)):abs(row_num-1),3], candles_filtered[abs(row_num-14):abs(row_num-1),4]))\r\natr_calc <- function(high.df, low.df){ return(sum(high.df - low.df) / nrow(high.df)) }\r\nThe ATR will be part of how we will be classifying the raw text.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-19T22:54:28-06:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Forex Senior Project",
    "description": "Welcome to our new blog, Forex Senior Project. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Robert Hess",
        "url": {}
      }
    ],
    "date": "2021-04-29",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-04-29T09:25:19-06:00",
    "input_file": {}
  }
]
